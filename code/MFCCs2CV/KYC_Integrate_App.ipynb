{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77738fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音频信号处理和可视化需要的包\n",
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import librosa\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# PyQt相关的包\n",
    "from PyQt5.QtWidgets import *\n",
    "from PyQt5.QtCore import *\n",
    "from PyQt5.QtMultimedia import QMediaContent, QMediaPlayer\n",
    "from PyQt5.QtGui import *\n",
    "# Tensorflow相关的包\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras as keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fc5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer块\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# 构建模型\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, num_classes, dropout_rate):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_projection = layers.Dense(embed_dim)  # 使用全连接层来调整输入维度\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate) \n",
    "                                   for _ in range(2)]\n",
    "        self.global_average = layers.GlobalAveragePooling1D()\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.out = layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.input_projection(inputs)  # 调整输入维度\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training)\n",
    "        x = self.global_average(x)  # 平均池化\n",
    "        x = self.dropout(x, training)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "def VitModel(input_shape, num_classes, patch_size, num_patches, projection_dim, num_heads, transformer_units, transformer_layers, mlp_head_units):\n",
    "    # 创建 ViT 模型\n",
    "    # 输入层\n",
    "    inputs = Input(shape=input_shape)\n",
    "    #     print(f\"输入层尺寸: {inputs.shape}\")  # 输入层尺寸: (?, 130, 13)\n",
    "    \n",
    "    # 将输入分割成 patches\n",
    "    patches = tf.keras.layers.Reshape((num_patches, patch_size * input_shape[-1]))(inputs)\n",
    "    #     print(f\"patches尺寸: {patches.shape}\")  # patches尺寸: (?, 10, 169)\n",
    "    \n",
    "    # Patch projection\n",
    "    x = Dense(units=projection_dim)(patches)\n",
    "    #     print(f\"Patch projection尺寸: {x.shape}\")  # Patch projection尺寸: (?, 10, 64)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    #     print(f\"LayerNormalization后尺寸: {x.shape}\")  # LayerNormalization后尺寸: (?, 10, 64)\n",
    "    x = Dropout(0.1)(x)\n",
    "    #     print(f\"Dropout后尺寸: {x.shape}\")  # Dropout后尺寸: (?, 10, 64)\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(transformer_layers):\n",
    "        # Multi-head attention and skip connection\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x, x)\n",
    "        #         print(f\"MultiHeadAttention后尺寸: {attention_output.shape}\")  # MultiHeadAttention后尺寸: (?, 10, 64)\n",
    "        attention_output = Add()([attention_output, x])\n",
    "        #         print(f\"Add后尺寸: {attention_output.shape}\")  # Add后尺寸: (?, 10, 64)\n",
    "        attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "        #         print(f\"LayerNormalization后尺寸: {attention_output.shape}\")  # LayerNormalization后尺寸: (?, 10, 64)\n",
    "        \n",
    "        # Feedforward network and skip connection\n",
    "        ffn_output = Dense(units=transformer_units[0], activation=\"relu\")(attention_output)\n",
    "        #         print(f\"Feedforward network后尺寸: {ffn_output.shape}\")  # Feedforward network后尺寸: (?, 10, 128)\n",
    "        ffn_output = Dense(units=projection_dim)(ffn_output)\n",
    "        #         print(f\"Dense后尺寸: {ffn_output.shape}\")  # Dense后尺寸: (?, 10, 64)\n",
    "        ffn_output = Dropout(0.1)(ffn_output)\n",
    "        #         print(f\"Dropout后尺寸: {ffn_output.shape}\")  # Dropout后尺寸: (?, 10, 64)\n",
    "        ffn_output = Add()([ffn_output, attention_output])\n",
    "        #         print(f\"Add后尺寸: {ffn_output.shape}\")  # Add后尺寸: (?, 10, 64)\n",
    "        x = LayerNormalization(epsilon=1e-6)(ffn_output)\n",
    "        #         print(f\"LayerNormalization后尺寸: {x.shape}\")  # LayerNormalization后尺寸: (?, 10, 64)\n",
    "    \n",
    "    # Representation layer\n",
    "    representation = GlobalAveragePooling1D()(x)\n",
    "    #     print(f\"GlobalAveragePooling1D后尺寸: {representation.shape}\")  # GlobalAveragePooling1D后尺寸: (?, 64)\n",
    "    \n",
    "    # MLP head\n",
    "    for units in mlp_head_units:\n",
    "        representation = Dense(units=units, activation=\"relu\")(representation)\n",
    "        #         print(f\"Dense后尺寸: {representation.shape}\")  # Dense后尺寸: (?, 64) 或 (?, 32) 取决于当前层\n",
    "\n",
    "    # 分类层\n",
    "    outputs = Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "    #     print(f\"分类层尺寸: {outputs.shape}\")  # 分类层尺寸: (?, 10)\n",
    "\n",
    "    # 创建模型\n",
    "    vit_classifier = Model(inputs=inputs, outputs=outputs)\n",
    "    return vit_classifier    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954b3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_file = \"01_data_user/genuine/test.wav\"  # 录音文件存储路径，或者加载的音频文件路径\n",
    "        self.dataset_path = \"01_data_user\" \n",
    "        self.json_path = \"data_add_user.json\"  # 音频文件预处理，输出到的json文件路径\n",
    "        self.model_path = '02_model/1205_5s.h5'  # 训练好的模型文件路径\n",
    "        self.robot_img = './fig/robot.png'\n",
    "        self.human_img = './fig/human.png'\n",
    "        self.unknown_img = './fig/Unknown.png'\n",
    "        self.initUI()\n",
    "        self.show()\n",
    "\n",
    "    def initUI(self):\n",
    "        # 设置窗口标题和初始大小\n",
    "        # font = QFont('Arial', 20)\n",
    "        self.setWindowTitle('真人语音/合成音频检测')\n",
    "        self.setGeometry(100, 100, 800, 600) \n",
    "        self.setFont(QFont('Times', 15))\n",
    "\n",
    "        # 创建文本编辑控件\n",
    "        self.text_edit = QTextEdit()\n",
    "        self.text_edit.setPlainText('start')  # 初始时向文本控件中插入'start'语句\n",
    "        self.text_edit.setFont(QFont('Times', 20))\n",
    "\n",
    "        # 创建按钮\n",
    "        self.button1 = QPushButton('录音')\n",
    "        self.button1.setFont(QFont('Times', 15))\n",
    "        self.button3 = QPushButton('加载音频文件')\n",
    "        self.button3.setFont(QFont('Times', 15))\n",
    "        self.button4 = QPushButton('播放音频')\n",
    "        self.button4.setFont(QFont('Times', 15))\n",
    "        self.button2 = QPushButton('检测')\n",
    "        self.button2.setFont(QFont('Times', 15))\n",
    "        \n",
    "        \n",
    "        self.label = QLabel()\n",
    "        self.label.setText('检测结果')\n",
    "        self.label.setAlignment(Qt.AlignCenter)\n",
    "        self.label.setMaximumHeight(200 // 5)\n",
    "\n",
    "        self.img = QLabel()\n",
    "        pixmap = QPixmap(self.unknown_img)\n",
    "        self.img.setPixmap(pixmap)\n",
    "        self.img.setAlignment(Qt.AlignCenter) \n",
    "        self.img.setScaledContents(False)\n",
    "\n",
    "        \n",
    "        # 创建一个垂直布局用于文本编辑控件\n",
    "        v_layout = QVBoxLayout()\n",
    "        v_layout.addWidget(self.label)\n",
    "        v_layout.addWidget(self.img)\n",
    "   \n",
    "        h_layout1 = QHBoxLayout()\n",
    "        h_layout1.addWidget(self.text_edit)\n",
    "        h_layout1.addLayout(v_layout)\n",
    "       \n",
    "        # 创建一个水平布局用于按钮\n",
    "        h_layout = QGridLayout(self)\n",
    "        h_layout.addWidget(self.button3, 0, 0)\n",
    "        \n",
    "        h_layout.addWidget(self.button1, 0, 1)\n",
    "        h_layout.addWidget(self.button4, 1, 0)\n",
    "        h_layout.addWidget(self.button2, 1, 1)\n",
    "\n",
    "        # 将水平和垂直布局添加到主布局中\n",
    "        main_layout = QVBoxLayout()\n",
    "        main_layout.addLayout(h_layout1)\n",
    "        main_layout.addLayout(h_layout)\n",
    "\n",
    "        # 创建一个中心小部件并设置布局\n",
    "        central_widget = QWidget()\n",
    "        central_widget.setLayout(main_layout)\n",
    "\n",
    "        # 设置中心小部件\n",
    "        self.setCentralWidget(central_widget)\n",
    "\n",
    "        # 连接按钮的点击信号到槽函数\n",
    "        self.button1.clicked.connect(self.on_button1_clicked)\n",
    "        self.button2.clicked.connect(self.on_button2_clicked)\n",
    "        self.button3.clicked.connect(self.on_button3_clicked)\n",
    "        self.button4.clicked.connect(self.on_button4_clicked)\n",
    "        \n",
    "    def on_button1_clicked(self):\n",
    "        # 添加录音事件处理逻辑\n",
    "        self.text_edit.append('启动录音')\n",
    "        self.start_audio()\n",
    "        self.text_edit.append('录音完成')\n",
    "\n",
    "    def on_button2_clicked(self):\n",
    "        # 添加检测事件处理逻辑\n",
    "        self.text_edit.append('启动检测')\n",
    "        # res_str = self.detect_audio()\n",
    "        res_str, label = self.detect_audio_single()\n",
    "        self.text_edit.append('检测完成：\\n'+res_str)\n",
    "        if label == 0:\n",
    "            pixmap = QPixmap(self.human_img)\n",
    "            self.img.setPixmap(pixmap)\n",
    "        elif label == 1:\n",
    "            pixmap = QPixmap(self.robot_img)\n",
    "            self.img.setPixmap(pixmap)\n",
    "\n",
    "    def on_button3_clicked(self):\n",
    "        # 打开文件对话框并获取选择的文件路径\n",
    "        options = QFileDialog.Options()\n",
    "        options |= QFileDialog.DontUseNativeDialog\n",
    "        file_name, _ = QFileDialog.getOpenFileName(self, \"QFileDialog.getOpenFileName()\", \"\",\n",
    "                                                  \"All Files (*);;WAV Files (*.wav)\", options=options)\n",
    "        if file_name:\n",
    "            # print(f\"选择的文件路径: {file_name}\")\n",
    "            self.save_file = file_name\n",
    "            self.text_edit.append(\"Select file:\"+file_name)\n",
    "            \n",
    "    def on_button4_clicked(self):\n",
    "        # 播放音频\n",
    "        # 创建媒体播放器对象\n",
    "        self.mediaPlayer = QMediaPlayer(self)        \n",
    "        # 设置音频文件路径\n",
    "        url = QUrl.fromLocalFile(self.save_file)        \n",
    "        # 加载音频文件\n",
    "        self.mediaPlayer.setMedia(QMediaContent(url))\n",
    "        # 播放音频\n",
    "        self.mediaPlayer.play()\n",
    "\n",
    "    ######################################以下为功能函数#################################################\n",
    "    def start_audio(self, time = 5, save_file=\"01_data_user/genuine/test.wav\"):\n",
    "        # 录音功能\n",
    "        CHUNK = 1024\n",
    "        FORMAT = pyaudio.paInt16\n",
    "        CHANNELS = 2\n",
    "        RATE = 16000\n",
    "        RECORD_SECONDS = time  #需要录制的时间\n",
    "        WAVE_OUTPUT_FILENAME = save_file #保存的文件名\n",
    "        p = pyaudio.PyAudio() #初始化\n",
    "\n",
    "        print(\"ON\")\n",
    "\n",
    "        stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)#创建录音文件\n",
    "        frames = []\n",
    "\n",
    "        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)#开始录音\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        print(\"OFF\")\n",
    "\n",
    "        wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\t#保存\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        self.save_file = save_file\n",
    "    \n",
    "    def save_mfcc(self, dataset_path, json_path, n_mfcc = 13, n_fft = 2048, hop_length = 512, num_segments = 5):\n",
    "        # 将音频文件转为MFCC\n",
    "        SAMPLE_RATE = 22050\n",
    "        DURATION = 5 # measured in seconds\n",
    "        SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "\n",
    "        data = {\n",
    "            \"mapping\" : [],\n",
    "            \"mfcc\" : [],\n",
    "            \"labels\" : []\n",
    "        }\n",
    "\n",
    "        num_samples_per_segments = int(SAMPLES_PER_TRACK / num_segments)\n",
    "        expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segments / hop_length) # 1.2 -> 2\n",
    "\n",
    "        # loop through all the genres\n",
    "        for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "            # ensure that we're not at the root level\n",
    "            if dirpath is not dataset_path:\n",
    "\n",
    "                # save the semantic label\n",
    "                dirpath_components = dirpath.split(\"/\") # genre/blues => [\"genre\", \"blues\"]\n",
    "                semantic_label = dirpath_components[-1]\n",
    "                data[\"mapping\"].append(semantic_label)\n",
    "   \n",
    "                for f in filenames:\n",
    "                    file_path = os.path.join(dirpath, f)\n",
    "                    signal, sr = librosa.load(file_path, sr = SAMPLE_RATE)\n",
    "                    for s in range(num_segments):\n",
    "                        start_sample = num_samples_per_segments * s # s = 0 -> 0\n",
    "                        finish_sample = start_sample + num_samples_per_segments # s = 0 -> num_samples_per_segments\n",
    "\n",
    "                        mfcc = librosa.feature.mfcc(y = signal[start_sample:finish_sample],\n",
    "                                                    sr = sr,\n",
    "                                                    n_fft = n_fft,\n",
    "                                                    n_mfcc = n_mfcc,\n",
    "                                                    hop_length = hop_length\n",
    "                                                    )\n",
    "                        mfcc = mfcc.T\n",
    "                        if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                            data[\"mfcc\"].append(mfcc.tolist())\n",
    "                            data[\"labels\"].append(i - 1) # first i is data_path itself\n",
    "        with open(json_path, \"w\") as fp:\n",
    "            json.dump(data, fp, indent = 4)\n",
    "    \n",
    "    \n",
    "    def save_mfcc_single(self, file_path, json_path, n_mfcc = 13, n_fft = 2048, hop_length = 512, num_segments = 5):\n",
    "        # 将选定的音频文件转为MFCC\n",
    "        SAMPLE_RATE = 22050\n",
    "        DURATION = 5 # measured in seconds\n",
    "        SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "        data = {\n",
    "            \"mapping\" : [],\n",
    "            \"mfcc\" : [],\n",
    "        }\n",
    "\n",
    "        num_samples_per_segments = int(SAMPLES_PER_TRACK / num_segments)\n",
    "        expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segments / hop_length) # 1.2 -> 2\n",
    "        \n",
    "        dirpath_components = file_path.split(\"/\") # genre/blues => [\"genre\", \"blues\"]\n",
    "        semantic_label = dirpath_components[-2]\n",
    "        data[\"mapping\"].append(semantic_label)\n",
    "   \n",
    "        print(\"file path is:\", file_path)\n",
    "        signal, sr = librosa.load(file_path, sr = SAMPLE_RATE)\n",
    "        for s in range(num_segments):\n",
    "            start_sample = num_samples_per_segments * s # s = 0 -> 0\n",
    "            finish_sample = start_sample + num_samples_per_segments # s = 0 -> num_samples_per_segments\n",
    "\n",
    "            mfcc = librosa.feature.mfcc(y = signal[start_sample:finish_sample],\n",
    "                                        sr = sr,\n",
    "                                        n_fft = n_fft,\n",
    "                                        n_mfcc = n_mfcc,\n",
    "                                        hop_length = hop_length\n",
    "                                        )\n",
    "            mfcc = mfcc.T\n",
    "            if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                data[\"mfcc\"].append(mfcc.tolist())\n",
    "        with open(json_path, \"w\") as fp:\n",
    "            json.dump(data, fp, indent = 4)\n",
    "            \n",
    "    #################################以下为深度学习功能函数#################################################\n",
    "    def load_data(self, data_path):\n",
    "        # 加载数据集\n",
    "        \"\"\"Loads training dataset from json file\n",
    "\n",
    "            :param data_path (str): Path to json file containing data\n",
    "            :return X (ndarray): Inputs\n",
    "            :return y (ndarray): Targets\n",
    "\n",
    "        \"\"\"\n",
    "        with open(data_path, \"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "\n",
    "        X = np.array(data[\"mfcc\"])\n",
    "        attr = 'labels'\n",
    "        if attr in data:\n",
    "            y = np.array(data[\"labels\"])\n",
    "        else:\n",
    "            y = None\n",
    "        return X, y\n",
    "    \n",
    "    def load_data_single(self, data_path):\n",
    "        # 加载单个数据\n",
    "        \"\"\"Loads training dataset from json file\n",
    "\n",
    "            :param data_path (str): Path to json file containing data\n",
    "            :return X (ndarray): Inputs\n",
    "            :return y (ndarray): Targets\n",
    "\n",
    "        \"\"\"\n",
    "        with open(data_path, \"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "\n",
    "        X = np.array(data[\"mfcc\"])\n",
    "        y = None\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def build_LSTM(self, input_shape):\n",
    "        # 构建LSTM模型\n",
    "        model = keras.Sequential()\n",
    "        # 3 LSTM layers\n",
    "        model.add(keras.layers.LSTM(512, input_shape = input_shape, return_sequences = True))\n",
    "        model.add(keras.layers.LSTM(256))\n",
    "        # dense layers\n",
    "        model.add(keras.layers.Dense(128, activation = 'relu'))\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "        # dense layers\n",
    "        model.add(keras.layers.Dense(32, activation = 'relu'))\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "        # output layer\n",
    "        model.add(keras.layers.Dense(2, activation = 'softmax'))\n",
    "        return model\n",
    "    \n",
    "    def build_ViT(self):\n",
    "        # 定义 ViT 参数\n",
    "        input_shape = (44, 13)  # 输入数据的形状\n",
    "        num_classes = 2  # 分类数量\n",
    "        patch_size = 4  # 每个patch的特征数量\n",
    "        num_patches = input_shape[0] // patch_size  # 计算patches的数量\n",
    "        projection_dim = 64  # patch projection的维度\n",
    "        num_heads = 4  # Transformer中的头数\n",
    "        transformer_units = [128, 64]  # Transformer中的前馈网络单元\n",
    "        transformer_layers = 2  # Transformer的层数\n",
    "        mlp_head_units = [64, 32]  # MLP头部的单元\n",
    "\n",
    "        # 创建 ViT 模型实例\n",
    "        model = create_vit_classifier(input_shape, num_classes, patch_size, num_patches, projection_dim, num_heads, transformer_units, transformer_layers, mlp_head_units)\n",
    "        return model\n",
    "    \n",
    "    def build_Transformer(self):\n",
    "        # 创建Transformer\n",
    "        # 参数设置\n",
    "        embed_dim = 64  # 嵌入维度\n",
    "        num_heads = 4    # 多头注意力的头数\n",
    "        ff_dim = 256     # 前馈神经网络的维度\n",
    "        num_classes = 2 # 分类类别数，根据实际数据集调整\n",
    "        dropout_rate = 0 # Dropout比率\n",
    "        # 实例化模型\n",
    "        model = TransformerModel(embed_dim, num_heads, ff_dim, num_classes, dropout_rate)\n",
    "        return model\n",
    "    \n",
    "    ###########################################################################################\n",
    "    def detect_audio_single(self):    \n",
    "        # 检测音频\n",
    "        file_path = self.save_file\n",
    "        json_path = self.json_path\n",
    "        self.save_mfcc_single(file_path, json_path)\n",
    "        ### 使用LSTM模型参数\n",
    "        model=keras.models.load_model(self.model_path)\n",
    "        ### 使用Transformer模型参数\n",
    "        # model = self.build_Transformer()\n",
    "        # model.build(input_shape=(None, 44, 13))\n",
    "        # model.load_weights('02_model/Transformer-mini.h5')\n",
    "        ### 使用ViT模型参数\n",
    "        # model= self.build_Vit()\n",
    "        # model.build(input_shape=(None, 44, 13))\n",
    "        # model.load_weights('02_model/Vit.h5')\n",
    "        \n",
    "        # load data\n",
    "        X, y = self.load_data_single(json_path)\n",
    "        y_pre_prob = model.predict(X)\n",
    "        y_pre = np.argmax(y_pre_prob, axis = 1)\n",
    "        if (all(y_pre_id == 1 for y_pre_id in y_pre)):\n",
    "            print(\"发音是真人!\\n\\n\")\n",
    "            return '发音是真人\\n', 0\n",
    "\n",
    "        else:\n",
    "            print(\"声音很可能是合成!\\n\\n\")\n",
    "            return \"声音很可能是合成!\\n\", 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6d823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path is: E:/Fintech/Current_Code/01_data_user/genuine/XZY.wav\n",
      "1/1 [==============================] - 1s 956ms/step\n",
      "声音很可能是合成!\n",
      "\n",
      "\n",
      "file path is: E:/Fintech/Current_Code/01_data_user/genuine/XZY2.wav\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "声音很可能是合成!\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\YOLOv5_new_GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 创建应用程序实例\n",
    "app = QApplication(sys.argv)\n",
    "window = MainWindow()\n",
    "window.show()\n",
    "# 运行应用程序\n",
    "sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b520d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
